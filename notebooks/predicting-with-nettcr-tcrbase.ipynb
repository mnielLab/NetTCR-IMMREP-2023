{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e87cfb78",
   "metadata": {
    "papermill": {
     "duration": 0.005991,
     "end_time": "2023-12-11T14:59:36.718390",
     "exception": false,
     "start_time": "2023-12-11T14:59:36.712399",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Predicting with NetTCR/TCRbase #\n",
    "This notebook shows an example of how to use a NetTCR model trained with nested cross-validation (5 partitions) to perform predictions on the IMMREP test data.\n",
    "\n",
    "In this example, we predict with the ensemble of models used for the M5 submission in the IMMREP 2023 competition.\n",
    "\n",
    "We start by loading the modules required for running the script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "887344f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-11T14:59:36.735076Z",
     "iopub.status.busy": "2023-12-11T14:59:36.733919Z",
     "iopub.status.idle": "2023-12-11T14:59:53.112981Z",
     "shell.execute_reply": "2023-12-11T14:59:53.111900Z"
    },
    "papermill": {
     "duration": 16.389634,
     "end_time": "2023-12-11T14:59:53.116156",
     "exception": false,
     "start_time": "2023-12-11T14:59:36.726522",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Loading Modules\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import subprocess\n",
    "import itertools\n",
    "import time\n",
    "import random\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "#Silence Tf logging\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3974503a",
   "metadata": {
    "papermill": {
     "duration": 0.004934,
     "end_time": "2023-12-11T14:59:53.126721",
     "exception": false,
     "start_time": "2023-12-11T14:59:53.121787",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Input Arguments ###\n",
    "In the following code, we specify the arguments using for performing the predictions.\n",
    "\n",
    "The \"infile\" parameter is used to define the file to predict on (e.g. the IMMREP 'test.csv' file)\n",
    "\n",
    "The \"model_path\" defines the path to a text file containing a list of model folders, which are to be used in the predictions. In this example, we use the ensemble of models used for the M5 prediction. See \"../data/model_ensemble_files/M5_v2.txt\" for an overview of what the model folder should contain after training.\n",
    "\n",
    "\"tcrbase_file\" contains the predictions from TCRbase on the IMMREP test data.\n",
    "\n",
    "\"outfile\" determines where the output file is saved to.\n",
    "\n",
    "\"alpha\" is used for scaling the CNN predictions by similarity to known binders (from the TCRbase prediction), by lifting the similarity to a power of alpha. If this is set to 0, the TCRbase scaling is disabled.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1e0b734",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-11T14:59:53.140579Z",
     "iopub.status.busy": "2023-12-11T14:59:53.139133Z",
     "iopub.status.idle": "2023-12-11T14:59:53.147230Z",
     "shell.execute_reply": "2023-12-11T14:59:53.146325Z"
    },
    "papermill": {
     "duration": 0.017945,
     "end_time": "2023-12-11T14:59:53.149832",
     "exception": false,
     "start_time": "2023-12-11T14:59:53.131887",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### For commandline ###\n",
    "\"\"\"\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"-i\", \"--infile\", default = \"/home/projects/vaccine/people/matjen/IMMREP_competition/data/test.csv\", help=\"Specify input file with peptide and all six CDR sequences\")\n",
    "parser.add_argument(\"-m\", \"--model_path\", help = \"Path to a file containing a newline separated list of model folders to use for predictions. The folder should have the subfolders 'cdr123_pan' and 'pre_trained'.\")\n",
    "parser.add_argument(\"-tb\", \"--tcrbase_file\", default = None)\n",
    "parser.add_argument(\"-o\", \"--outfile\", default=\"/home/projects/vaccine/people/matjen/IMMREP_competition/predictions/nettcr_predictions.csv\", help=\"Specify output file\")\n",
    "parser.add_argument(\"-a\", \"--alpha\", default = 10, help=\"Determines how much the final predictions takes similarity to known binders into account via TCRbase.\\nThe final prediction score is given by pred = CNN_pred*TCRbase_pred^alpha. An alpha of 0 disables TCRbase scaling\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "infile = str(args.infile)\n",
    "alpha = int(args.alpha)\n",
    "model_path = str(args.model_path)\n",
    "tcrbase_file = str(args.tcrbase_file)\n",
    "outfile = str(args.outfile)\n",
    "\"\"\"\n",
    "### For notebook ###\n",
    "infile = '../data/test.csv'\n",
    "model_path = '../data/model_ensemble_files/M5_v2.txt'\n",
    "tcrbase_file = '../models/TCRbase/alpha_beta/ensemble_limited/IMMREP_pred_df.csv'\n",
    "outfile = '../predictions/M5_v2_notebook_prediction.csv'\n",
    "alpha = 10 #Set to 0 to disable TCRbase scaling (alpha=0 in M1 and M2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e049160",
   "metadata": {
    "papermill": {
     "duration": 0.005092,
     "end_time": "2023-12-11T14:59:53.160398",
     "exception": false,
     "start_time": "2023-12-11T14:59:53.155306",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Utility functions and other parameters ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "333e5358",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-11T14:59:53.174013Z",
     "iopub.status.busy": "2023-12-11T14:59:53.172952Z",
     "iopub.status.idle": "2023-12-11T14:59:53.245498Z",
     "shell.execute_reply": "2023-12-11T14:59:53.244271Z"
    },
    "papermill": {
     "duration": 0.082781,
     "end_time": "2023-12-11T14:59:53.248519",
     "exception": false,
     "start_time": "2023-12-11T14:59:53.165738",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Utility functions/dictionaries\n",
    "def enc_list_bl_max_len(aa_seqs, blosum, max_seq_len, padding = \"right\"):\n",
    "    '''\n",
    "    blosum encoding of a list of amino acid sequences with padding \n",
    "    to a max length\n",
    "\n",
    "    parameters:\n",
    "        - aa_seqs : list with AA sequences\n",
    "        - blosum : dictionnary: key= AA, value= blosum encoding\n",
    "        - max_seq_len: common length for padding\n",
    "    returns:\n",
    "        - enc_aa_seq : list of np.ndarrays containing padded, encoded amino acid sequences\n",
    "    '''\n",
    "\n",
    "    # encode sequences:\n",
    "    sequences=[]\n",
    "    for seq in aa_seqs:\n",
    "        e_seq= np.zeros((len(seq),len(blosum[\"A\"])))\n",
    "        count=0\n",
    "        for aa in seq:\n",
    "            if aa in blosum:\n",
    "                e_seq[count]=blosum[aa]\n",
    "                count+=1\n",
    "            else:\n",
    "                sys.stderr.write(\"Unknown amino acid in peptides: \"+ aa +\", encoding aborted!\\n\")\n",
    "                sys.exit(2)\n",
    "                \n",
    "        sequences.append(e_seq)\n",
    "\n",
    "    # pad sequences:\n",
    "    #max_seq_len = max([len(x) for x in aa_seqs])\n",
    "    n_seqs = len(aa_seqs)\n",
    "    n_features = sequences[0].shape[1]\n",
    "\n",
    "    enc_aa_seq = -5*np.ones((n_seqs, max_seq_len, n_features))\n",
    "    if padding == \"right\":\n",
    "        for i in range(0,n_seqs):\n",
    "            enc_aa_seq[i, :sequences[i].shape[0], :n_features] = sequences[i]\n",
    "            \n",
    "    elif padding == \"left\":\n",
    "        for i in range(0,n_seqs):\n",
    "            enc_aa_seq[i, max_seq_len-sequences[i].shape[0]:max_seq_len, :n_features] = sequences[i]\n",
    "    \n",
    "    else:\n",
    "        print(\"Error: No valid padding has been chosen.\\nValid options: 'right', 'left'\")\n",
    "        \n",
    "\n",
    "    return enc_aa_seq\n",
    "\n",
    "\n",
    "########################\n",
    "### Encoding schemes ###\n",
    "########################\n",
    "\n",
    "blosum50_20aa = {\n",
    "        'A': np.array((5,-2,-1,-2,-1,-1,-1,0,-2,-1,-2,-1,-1,-3,-1,1,0,-3,-2,0)),\n",
    "        'R': np.array((-2,7,-1,-2,-4,1,0,-3,0,-4,-3,3,-2,-3,-3,-1,-1,-3,-1,-3)),\n",
    "        'N': np.array((-1,-1,7,2,-2,0,0,0,1,-3,-4,0,-2,-4,-2,1,0,-4,-2,-3)),\n",
    "        'D': np.array((-2,-2,2,8,-4,0,2,-1,-1,-4,-4,-1,-4,-5,-1,0,-1,-5,-3,-4)),\n",
    "        'C': np.array((-1,-4,-2,-4,13,-3,-3,-3,-3,-2,-2,-3,-2,-2,-4,-1,-1,-5,-3,-1)),\n",
    "        'Q': np.array((-1,1,0,0,-3,7,2,-2,1,-3,-2,2,0,-4,-1,0,-1,-1,-1,-3)),\n",
    "        'E': np.array((-1,0,0,2,-3,2,6,-3,0,-4,-3,1,-2,-3,-1,-1,-1,-3,-2,-3)),\n",
    "        'G': np.array((0,-3,0,-1,-3,-2,-3,8,-2,-4,-4,-2,-3,-4,-2,0,-2,-3,-3,-4)),\n",
    "        'H': np.array((-2,0,1,-1,-3,1,0,-2,10,-4,-3,0,-1,-1,-2,-1,-2,-3,2,-4)),\n",
    "        'I': np.array((-1,-4,-3,-4,-2,-3,-4,-4,-4,5,2,-3,2,0,-3,-3,-1,-3,-1,4)),\n",
    "        'L': np.array((-2,-3,-4,-4,-2,-2,-3,-4,-3,2,5,-3,3,1,-4,-3,-1,-2,-1,1)),\n",
    "        'K': np.array((-1,3,0,-1,-3,2,1,-2,0,-3,-3,6,-2,-4,-1,0,-1,-3,-2,-3)),\n",
    "        'M': np.array((-1,-2,-2,-4,-2,0,-2,-3,-1,2,3,-2,7,0,-3,-2,-1,-1,0,1)),\n",
    "        'F': np.array((-3,-3,-4,-5,-2,-4,-3,-4,-1,0,1,-4,0,8,-4,-3,-2,1,4,-1)),\n",
    "        'P': np.array((-1,-3,-2,-1,-4,-1,-1,-2,-2,-3,-4,-1,-3,-4,10,-1,-1,-4,-3,-3)),\n",
    "        'S': np.array((1,-1,1,0,-1,0,-1,0,-1,-3,-3,0,-2,-3,-1,5,2,-4,-2,-2)),\n",
    "        'T': np.array((0,-1,0,-1,-1,-1,-1,-2,-2,-1,-1,-1,-1,-2,-1,2,5,-3,-2,0)),\n",
    "        'W': np.array((-3,-3,-4,-5,-5,-1,-3,-3,-3,-3,-2,-3,-1,1,-4,-4,-3,15,2,-3)),\n",
    "        'Y': np.array((-2,-1,-2,-3,-3,-1,-2,-3,2,-1,-1,-2,0,4,-3,-2,-2,2,8,-1)),\n",
    "        'V': np.array((0,-3,-3,-4,-1,-3,-3,-4,-4,4,1,-3,1,-1,-3,-2,0,-3,-1,5))\n",
    "    }\n",
    "\n",
    "blosum50_20aa_masking = {\n",
    "        'A': np.array((5,-2,-1,-2,-1,-1,-1,0,-2,-1,-2,-1,-1,-3,-1,1,0,-3,-2,0)),\n",
    "        'R': np.array((-2,7,-1,-2,-4,1,0,-3,0,-4,-3,3,-2,-3,-3,-1,-1,-3,-1,-3)),\n",
    "        'N': np.array((-1,-1,7,2,-2,0,0,0,1,-3,-4,0,-2,-4,-2,1,0,-4,-2,-3)),\n",
    "        'D': np.array((-2,-2,2,8,-4,0,2,-1,-1,-4,-4,-1,-4,-5,-1,0,-1,-5,-3,-4)),\n",
    "        'C': np.array((-1,-4,-2,-4,13,-3,-3,-3,-3,-2,-2,-3,-2,-2,-4,-1,-1,-5,-3,-1)),\n",
    "        'Q': np.array((-1,1,0,0,-3,7,2,-2,1,-3,-2,2,0,-4,-1,0,-1,-1,-1,-3)),\n",
    "        'E': np.array((-1,0,0,2,-3,2,6,-3,0,-4,-3,1,-2,-3,-1,-1,-1,-3,-2,-3)),\n",
    "        'G': np.array((0,-3,0,-1,-3,-2,-3,8,-2,-4,-4,-2,-3,-4,-2,0,-2,-3,-3,-4)),\n",
    "        'H': np.array((-2,0,1,-1,-3,1,0,-2,10,-4,-3,0,-1,-1,-2,-1,-2,-3,2,-4)),\n",
    "        'I': np.array((-1,-4,-3,-4,-2,-3,-4,-4,-4,5,2,-3,2,0,-3,-3,-1,-3,-1,4)),\n",
    "        'L': np.array((-2,-3,-4,-4,-2,-2,-3,-4,-3,2,5,-3,3,1,-4,-3,-1,-2,-1,1)),\n",
    "        'K': np.array((-1,3,0,-1,-3,2,1,-2,0,-3,-3,6,-2,-4,-1,0,-1,-3,-2,-3)),\n",
    "        'M': np.array((-1,-2,-2,-4,-2,0,-2,-3,-1,2,3,-2,7,0,-3,-2,-1,-1,0,1)),\n",
    "        'F': np.array((-3,-3,-4,-5,-2,-4,-3,-4,-1,0,1,-4,0,8,-4,-3,-2,1,4,-1)),\n",
    "        'P': np.array((-1,-3,-2,-1,-4,-1,-1,-2,-2,-3,-4,-1,-3,-4,10,-1,-1,-4,-3,-3)),\n",
    "        'S': np.array((1,-1,1,0,-1,0,-1,0,-1,-3,-3,0,-2,-3,-1,5,2,-4,-2,-2)),\n",
    "        'T': np.array((0,-1,0,-1,-1,-1,-1,-2,-2,-1,-1,-1,-1,-2,-1,2,5,-3,-2,0)),\n",
    "        'W': np.array((-3,-3,-4,-5,-5,-1,-3,-3,-3,-3,-2,-3,-1,1,-4,-4,-3,15,2,-3)),\n",
    "        'Y': np.array((-2,-1,-2,-3,-3,-1,-2,-3,2,-1,-1,-2,0,4,-3,-2,-2,2,8,-1)),\n",
    "        'V': np.array((0,-3,-3,-4,-1,-3,-3,-4,-4,4,1,-3,1,-1,-3,-2,0,-3,-1,5)),\n",
    "        'X': np.array((0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0))\n",
    "    }\n",
    "\n",
    "            \n",
    "def adjust_batch_size(obs, batch_size, threshold = 0.5):\n",
    "    \"\"\"Adjusts the batch size, so that the final batch is at least half full\"\"\"\n",
    "    if obs/batch_size < threshold:\n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        if (obs/batch_size % 1) >= threshold:\n",
    "            pass\n",
    "        else:\n",
    "            while (obs/batch_size % 1) < threshold and (obs/batch_size % 1) != 0:\n",
    "                batch_size += 1\n",
    "    return batch_size\n",
    "\n",
    "def make_tf_ds(df, encoding):\n",
    "    \"\"\"Encodes amino acid sequences using a BLOSUM50 matrix with a normalization factor of 5.\n",
    "    Sequences are right-padded with [-1x20] for each AA missing, compared to the maximum embedding \n",
    "    length for that given feature\n",
    "    \n",
    "    Additionally, the input is prepared for predictions, by loading the data into a list of numpy arrays\"\"\"\n",
    "    encoded_pep = enc_list_bl_max_len(df.Peptide, encoding, pep_max)/5\n",
    "    encoded_a1 = enc_list_bl_max_len(df.CDR1a, encoding, a1_max)/5\n",
    "    encoded_a2 = enc_list_bl_max_len(df.CDR2a, encoding, a2_max)/5\n",
    "    encoded_a3 = enc_list_bl_max_len(df.CDR3a, encoding, a3_max)/5\n",
    "    encoded_b1 = enc_list_bl_max_len(df.CDR1b, encoding, b1_max)/5\n",
    "    encoded_b2 = enc_list_bl_max_len(df.CDR2b, encoding, b2_max)/5\n",
    "    encoded_b3 = enc_list_bl_max_len(df.CDR3b, encoding, b3_max)/5\n",
    "    tf_ds = [np.float32(encoded_pep),\n",
    "             np.float32(encoded_a1), np.float32(encoded_a2), np.float32(encoded_a3), \n",
    "             np.float32(encoded_b1), np.float32(encoded_b2), np.float32(encoded_b3)]\n",
    "\n",
    "    return tf_ds\n",
    "\n",
    "def my_numpy_function(y_true, y_pred):\n",
    "    \"\"\"Implementation of AUC 0.1 metric for Tensorflow\"\"\"\n",
    "    try:\n",
    "        auc = roc_auc_score(y_true, y_pred, max_fpr = 0.1)\n",
    "    except ValueError:\n",
    "        auc = np.array([float(0)])\n",
    "    return auc\n",
    "\n",
    "#Custom metric for AUC 0.1\n",
    "def auc_01(y_true, y_pred):\n",
    "    \"\"\"Converts function to optimised tensorflow numpy function\"\"\"\n",
    "    auc_01 = tf.numpy_function(my_numpy_function, [y_true, y_pred], tf.float64)\n",
    "    return auc_01\n",
    "\n",
    "#Custom stop metric used for EarlyStopping      \n",
    "def stop_metric(y_true, y_pred):\n",
    "    \"\"\"Custom stop metric for early stopping, which considers both loss and AUC 0.1\"\"\"\n",
    "    auc_01 = tf.numpy_function(my_numpy_function, [y_true, y_pred], tf.float64)\n",
    "    bce_loss = tf.keras.losses.BinaryCrossentropy()(y_true, y_pred)\n",
    "    bce_loss = tf.cast(bce_loss, tf.float64)\n",
    "    return auc_01 - bce_loss*0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d91b015",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-11T14:59:53.261883Z",
     "iopub.status.busy": "2023-12-11T14:59:53.261325Z",
     "iopub.status.idle": "2023-12-11T14:59:53.269090Z",
     "shell.execute_reply": "2023-12-11T14:59:53.267873Z"
    },
    "papermill": {
     "duration": 0.017525,
     "end_time": "2023-12-11T14:59:53.271738",
     "exception": false,
     "start_time": "2023-12-11T14:59:53.254213",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Model parameters ###\n",
    "train_parts = {0, 1, 2, 3, 4} #Partitions\n",
    "encoding = blosum50_20aa_masking #Encoding for amino acid sequences\n",
    "\n",
    "#Padding to certain length\n",
    "a1_max = 7\n",
    "a2_max = 8\n",
    "a3_max = 22\n",
    "b1_max = 6\n",
    "b2_max = 7\n",
    "b3_max = 23\n",
    "pep_max = 12\n",
    "\n",
    "# Set random seed\n",
    "seed=15\n",
    "np.random.seed(seed)  # Numpy module.\n",
    "random.seed(seed)  # Python random module.\n",
    "tf.random.set_seed(seed) # Tensorflow random seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a24c254",
   "metadata": {
    "papermill": {
     "duration": 0.005076,
     "end_time": "2023-12-11T14:59:53.282335",
     "exception": false,
     "start_time": "2023-12-11T14:59:53.277259",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Loading the input files ###\n",
    "In the following code block, the input data is loaded, along with the TCRbase predictions and the list of models to include in the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf07b5cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-11T14:59:53.295793Z",
     "iopub.status.busy": "2023-12-11T14:59:53.295331Z",
     "iopub.status.idle": "2023-12-11T14:59:53.491093Z",
     "shell.execute_reply": "2023-12-11T14:59:53.489483Z"
    },
    "papermill": {
     "duration": 0.206216,
     "end_time": "2023-12-11T14:59:53.494242",
     "exception": false,
     "start_time": "2023-12-11T14:59:53.288026",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Input ###\n",
    "# Read in data\n",
    "full_data = pd.read_csv(infile)\n",
    "\n",
    "### Get peptides in input file ###\n",
    "pep_list = list(full_data.Peptide.value_counts(ascending=False).index)\n",
    "\n",
    "#Prepare output DataFrame (test predictions)\n",
    "full_pred_df = pd.DataFrame()\n",
    "\n",
    "#Necessary to load the model with the custom metric\n",
    "dependencies = {\n",
    "    'auc_01': auc_01,\n",
    "    \"stop_metric\": stop_metric\n",
    "}\n",
    "\n",
    "if tcrbase_file is not None:\n",
    "    tcrbase_df = pd.read_csv(tcrbase_file)\n",
    "    \n",
    "model_list = []\n",
    "with open(model_path, \"r\") as infile: \n",
    "    model_list = infile.readlines()\n",
    "model_list = [x.strip() for x in model_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5c71c5",
   "metadata": {
    "papermill": {
     "duration": 0.005167,
     "end_time": "2023-12-11T14:59:53.505101",
     "exception": false,
     "start_time": "2023-12-11T14:59:53.499934",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Predictions ##\n",
    "With the data loaded, we are now ready to perform our predictions.\n",
    "\n",
    "If a pre-trained model exists for a given peptide (meaning that we have training data for it), this model will be used for predictions. \n",
    "\n",
    "However, it the peptide is unseen, we revert to using a pan-specific model for the predictions (and we here expect a considerably lower performance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5829a03f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-11T14:59:53.519080Z",
     "iopub.status.busy": "2023-12-11T14:59:53.517958Z",
     "iopub.status.idle": "2023-12-11T15:00:18.841703Z",
     "shell.execute_reply": "2023-12-11T15:00:18.838529Z"
    },
    "papermill": {
     "duration": 25.334439,
     "end_time": "2023-12-11T15:00:18.844963",
     "exception": false,
     "start_time": "2023-12-11T14:59:53.510524",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions for  ../predictions/M5_v2_notebook_prediction.csv\n",
      "Using the following files:\n",
      "/models/NetTCR_2_3/paired/ensemble_limited/15\n",
      "/models/NetTCR_2_3/paired/ensemble_limited/20\n",
      "/models/NetTCR_2_3/paired/ensemble_limited/30\n",
      "/models/NetTCR_2_3/paired/ensemble_limited/40\n",
      "/models/NetTCR_2_3/paired/ensemble_limited/50\n",
      "/models/NetTCR_2_3/alpha/ensemble_limited/15\n",
      "/models/NetTCR_2_3/alpha/ensemble_limited/20\n",
      "/models/NetTCR_2_3/alpha/ensemble_limited/30\n",
      "/models/NetTCR_2_3/alpha/ensemble_limited/40\n",
      "/models/NetTCR_2_3/alpha/ensemble_limited/50\n",
      "/models/NetTCR_2_3/beta/ensemble_limited/15\n",
      "/models/NetTCR_2_3/beta/ensemble_limited/20\n",
      "/models/NetTCR_2_3/beta/ensemble_limited/30\n",
      "/models/NetTCR_2_3/beta/ensemble_limited/40\n",
      "/models/NetTCR_2_3/beta/ensemble_limited/50\n",
      "/models/NetTCR_2_3/mixed/ensemble_limited/15\n",
      "/models/NetTCR_2_3/mixed/ensemble_limited/20\n",
      "/models/NetTCR_2_3/mixed/ensemble_limited/30\n",
      "/models/NetTCR_2_3/mixed/ensemble_limited/40\n",
      "/models/NetTCR_2_3/mixed/ensemble_limited/50\n",
      "Making predictions for GILGFVFTL- Predictions took 23.073 seconds\n",
      "\n",
      "Making predictions for RAKFKQLL- Predictions took 19.032 seconds\n",
      "\n",
      "Making predictions for VSDGGPNLY- Predictions took 18.503 seconds\n",
      "\n",
      "Making predictions for EPLPQGQLTAY- Predictions took 15.285 seconds\n",
      "\n",
      "Making predictions for NLVPMVATV- Predictions took 14.71 seconds\n",
      "\n",
      "Making predictions for YVLDHLIVV- Predictions took 12.995 seconds\n",
      "\n",
      "Making predictions for TDLGQNLLY. WARNING: A model for TDLGQNLLY does not exist. Using pan-specific model instead - Predictions took 14.375 seconds\n",
      "\n",
      "Making predictions for VTEHDTLLY. WARNING: A model for VTEHDTLLY does not exist. Using pan-specific model instead - Predictions took 9.312 seconds\n",
      "\n",
      "Making predictions for GLCTLVAML- Predictions took 10.635 seconds\n",
      "\n",
      "Making predictions for VLEETSVML. WARNING: A model for VLEETSVML does not exist. Using pan-specific model instead - Predictions took 9.147 seconds\n",
      "\n",
      "Making predictions for RPHERNGFTVL- Predictions took 9.519 seconds\n",
      "\n",
      "Making predictions for SALPTNADLY. WARNING: A model for SALPTNADLY does not exist. Using pan-specific model instead - Predictions took 7.083 seconds\n",
      "\n",
      "Making predictions for IPSINVHHY- Predictions took 8.328 seconds\n",
      "\n",
      "Making predictions for QIKVRVDMV- Predictions took 8.271 seconds\n",
      "\n",
      "Making predictions for RPPIFIRRL- Predictions took 8.273 seconds\n",
      "\n",
      "Making predictions for IVTDFSVIK- Predictions took 7.243 seconds\n",
      "\n",
      "Making predictions for TPRVTGGGAM- Predictions took 7.388 seconds\n",
      "\n",
      "Making predictions for YLQPRTFLL- Predictions took 7.29 seconds\n",
      "\n",
      "Making predictions for FTDALGIDEY. WARNING: A model for FTDALGIDEY does not exist. Using pan-specific model instead - Predictions took 5.319 seconds\n",
      "\n",
      "Making predictions for TSDACMMTMY. WARNING: A model for TSDACMMTMY does not exist. Using pan-specific model instead - Predictions took 5.047 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Predictions\n",
    "print(\"Making predictions for \", outfile)\n",
    "print(\"Using the following files:\")\n",
    "print(\"\\n\".join(model_list))\n",
    "for pep in pep_list:\n",
    "    time_start = time.time()\n",
    "    pred_df = full_data[full_data.Peptide == pep].copy(deep = True)\n",
    "    test_tensor = make_tf_ds(pred_df, encoding = encoding)\n",
    "    \n",
    "    tcrbase_pep_df = tcrbase_df[tcrbase_df.Peptide == pep].copy(deep = True)\n",
    "    \n",
    "    #Flag for scaling with TCRbase\n",
    "    scale_prediction = False\n",
    "    \n",
    "    #Used for announcing that a model does not exist for the given peptide\n",
    "    print_flag = 0\n",
    "    \n",
    "    print(\"Making predictions for {}\".format(pep), end = \"\")\n",
    "    if alpha != 0:\n",
    "        if tcrbase_pep_df.TCRbase_flag.unique()[0] == 0:\n",
    "            scale_prediction = False\n",
    "        else:\n",
    "            scale_prediction = True\n",
    "    \n",
    "    avg_prediction = 0\n",
    "    n_models = 0\n",
    "    for model in model_list:\n",
    "        n_models += 20\n",
    "        for t in train_parts:\n",
    "            x_test = test_tensor[0:7]\n",
    "            \n",
    "            for v in train_parts:\n",
    "                if v!=t:      \n",
    "                    \n",
    "                    # Load the TFLite model and allocate tensors.\n",
    "                    try:\n",
    "                        interpreter = tf.lite.Interpreter(model_path = \"..\" + model+'/pre_trained/'+pep+'/checkpoint/'+'t.'+str(t)+'.v.'+str(v)+\".tflite\")\n",
    "                    except ValueError as error:\n",
    "                        #print(error)\n",
    "                        print_flag += 1\n",
    "                        # Load pan-specific TFLite model and allocate tensors.\n",
    "                        interpreter = tf.lite.Interpreter(model_path = \"..\" + model+'/cdr123_pan/checkpoint/'+'t.'+str(t)+'.v.'+str(v)+\".tflite\")\n",
    "                        if print_flag == 1:\n",
    "                            print(\". WARNING: A model for {} does not exist. Using pan-specific model instead \".format(pep), end = \"\")\n",
    "                    \n",
    "                    # Get input and output tensors for the model.\n",
    "                    input_details = interpreter.get_input_details()\n",
    "                    output_details = interpreter.get_output_details()\n",
    "                    \n",
    "                    #Fix Output dimensions\n",
    "                    output_shape = output_details[0]['shape']\n",
    "                    interpreter.resize_tensor_input(output_details[0][\"index\"], [x_test[0].shape[0], output_details[0][\"shape\"][1]])\n",
    "                    \n",
    "                    #Fix Input dimensions\n",
    "                    for i in range(len(input_details)):\n",
    "                        interpreter.resize_tensor_input(input_details[i][\"index\"], [x_test[0].shape[0], input_details[i][\"shape\"][1], input_details[i][\"shape\"][2]])\n",
    "                    \n",
    "                    #Prepare tensors\n",
    "                    interpreter.allocate_tensors()\n",
    "                    \n",
    "                    data_dict = {\"pep\": x_test[0],\n",
    "                                 \"a1\": x_test[1],\n",
    "                                 \"a2\": x_test[2],\n",
    "                                 \"a3\": x_test[3],\n",
    "                                 \"b1\": x_test[4],\n",
    "                                 \"b2\": x_test[5],\n",
    "                                 \"b3\": x_test[6]}\n",
    "                    \n",
    "                    #Assign input data\n",
    "                    for i in range(len(input_details)):   \n",
    "                        #Set input data for a given feature based on the name of the input in \"input_details\"\n",
    "                        interpreter.set_tensor(input_details[i]['index'], data_dict[input_details[i][\"name\"].split(\":\")[0].split(\"_\")[-1]])\n",
    "                    \n",
    "                    #Prepare the model for predictions\n",
    "                    interpreter.invoke()\n",
    "    \n",
    "                    #Predict on input tensor\n",
    "                    avg_prediction += interpreter.get_tensor(output_details[0]['index'])\n",
    "        \n",
    "                    #Clears the session for the next model\n",
    "                    tf.keras.backend.clear_session()\n",
    "    \n",
    "    #Averaging the predictions between all models\n",
    "    avg_prediction = avg_prediction/n_models\n",
    "    \n",
    "    #Flatten list of predictions\n",
    "    avg_prediction = list(itertools.chain(*avg_prediction))\n",
    "    \n",
    "    #Run TCRbase if alpha is not set to 0, and a positive database for the peptide exists\n",
    "    if scale_prediction is True and tcrbase_file is not None:\n",
    "        pred_df['Prediction'] = avg_prediction * tcrbase_pep_df[\"Prediction\"] ** alpha  \n",
    "    else:\n",
    "        pred_df['Prediction'] = avg_prediction\n",
    "    \n",
    "    full_pred_df = pd.concat([full_pred_df, pred_df])\n",
    "    print(\"- Predictions took \"+str(round(time.time()-time_start, 3))+\" seconds\\n\")\n",
    "    \n",
    "#Save predictions in the same order as the input\n",
    "full_pred_df.sort_values(\"ID\", ascending = True, inplace = True)\n",
    "full_pred_df.to_csv(outfile, index=False, sep = ',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a1b6f4",
   "metadata": {
    "papermill": {
     "duration": 0.007118,
     "end_time": "2023-12-11T15:00:18.874533",
     "exception": false,
     "start_time": "2023-12-11T15:00:18.867415",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## License ##\n",
    "NetTCR-2.2 is freely available for academic user for non-commercial purposes (see license). The product is provided free of charge, and, therefore, on an \"as is\" basis, without warranty of any kind.\n",
    "Other users: If you plan to use NetTCR-2.2 or any data provided with the script in any for-profit application, you are required to obtain a separate license. To do so, please contact health-software@dtu.dk.\n",
    "\n",
    "For licence details refer to [***academic_software_license_agreement.pdf***](https://github.com/mnielLab/NetTCR-2.2/blob/main/academic_software_license_agreement.pdf)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 6890356,
     "sourceId": 54698,
     "sourceType": "competition"
    },
    {
     "datasetId": 4080782,
     "sourceId": 7083260,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4080836,
     "sourceId": 7083366,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30587,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 48.665629,
   "end_time": "2023-12-11T15:00:21.357923",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-12-11T14:59:32.692294",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
